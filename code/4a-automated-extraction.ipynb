{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaae6f7e",
   "metadata": {},
   "source": [
    "___\n",
    "# Automated extraction of IFI-related information\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "870dccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('ticks')\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['axes.titlesize'] = 22\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d82a2",
   "metadata": {},
   "source": [
    "___\n",
    "# Detect concepts and composite concepts\n",
    "### Run concept recognition and relationship detection on the development set with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3483103f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in each category: [29, 56, 10, 13, 12, 43, 32, 5, 9]\n",
      "Number of unique tokens in each category after expanding: [29, 56, 10, 15, 12, 43, 32, 5, 9]\n",
      "Number of unique tokens in each category: [28, 57, 13, 17, 11, 38, 31, 4, 9]\n",
      "Number of unique tokens in each category after expanding: [28, 57, 13, 17, 11, 38, 31, 4, 9]\n",
      "Number of unique tokens in each category: [31, 60, 14, 15, 13, 44, 33, 5, 9]\n",
      "Number of unique tokens in each category after expanding: [31, 60, 14, 15, 13, 44, 33, 5, 9]\n",
      "Number of unique tokens in each category: [26, 58, 14, 13, 13, 41, 30, 5, 9]\n",
      "Number of unique tokens in each category after expanding: [26, 58, 14, 14, 13, 41, 30, 5, 9]\n",
      "Number of unique tokens in each category: [30, 58, 14, 16, 13, 42, 33, 5, 8]\n",
      "Number of unique tokens in each category after expanding: [30, 58, 14, 17, 13, 42, 33, 5, 8]\n",
      "Number of unique tokens in each category: [30, 53, 14, 14, 12, 40, 32, 5, 7]\n",
      "Number of unique tokens in each category after expanding: [30, 53, 14, 16, 12, 40, 32, 5, 7]\n",
      "Number of unique tokens in each category: [27, 50, 13, 15, 12, 38, 29, 3, 9]\n",
      "Number of unique tokens in each category after expanding: [27, 50, 13, 15, 12, 38, 29, 3, 9]\n",
      "Number of unique tokens in each category: [32, 60, 14, 17, 13, 41, 33, 5, 9]\n",
      "Number of unique tokens in each category after expanding: [32, 60, 14, 17, 13, 41, 33, 5, 9]\n",
      "Number of unique tokens in each category: [31, 57, 14, 17, 13, 43, 30, 5, 9]\n",
      "Number of unique tokens in each category after expanding: [31, 57, 14, 17, 13, 43, 30, 5, 9]\n",
      "Number of unique tokens in each category: [29, 56, 13, 17, 12, 40, 31, 5, 9]\n",
      "Number of unique tokens in each category after expanding: [29, 56, 13, 17, 12, 40, 31, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "# Load the development set\n",
    "df = pd.read_csv(\"../datasets/reports_dev.csv\")\n",
    "\n",
    "# Clean data\n",
    "df['clean_text'] = df.report_text.apply(utils.clean_text)\n",
    "\n",
    "# Learn vocabulary from k-1 folds and extract concepts from the k-th fold\n",
    "df = utils.extract_features_cv(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3617baf9",
   "metadata": {},
   "source": [
    "### Run concept recognition and relationship detection on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32042157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in each category: [32, 61, 14, 17, 13, 44, 33, 5, 9]\n",
      "Number of unique tokens in each category after expanding: [32, 61, 14, 17, 13, 44, 33, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "# Load the test set\n",
    "df_test = pd.read_csv(\"../datasets/reports_test.csv\")\n",
    "\n",
    "# Clean data\n",
    "df_test['clean_text'] = df_test.report_text.apply(utils.clean_text)\n",
    "\n",
    "# Learn vocabulary from the development set\n",
    "vocab = utils.create_vocab(df.report_id, expand=True)\n",
    "\n",
    "# Extract concepts from the test set\n",
    "df_test = utils.extract_features(df_test, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb24c7f",
   "metadata": {},
   "source": [
    "___\n",
    "# Generate `.ann` files with detected concepts and composite concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e3026f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "5     None\n",
       "6     None\n",
       "7     None\n",
       "8     None\n",
       "9     None\n",
       "10    None\n",
       "11    None\n",
       "12    None\n",
       "13    None\n",
       "14    None\n",
       "15    None\n",
       "16    None\n",
       "17    None\n",
       "18    None\n",
       "19    None\n",
       "20    None\n",
       "21    None\n",
       "22    None\n",
       "23    None\n",
       "24    None\n",
       "25    None\n",
       "26    None\n",
       "27    None\n",
       "28    None\n",
       "29    None\n",
       "30    None\n",
       "31    None\n",
       "32    None\n",
       "33    None\n",
       "34    None\n",
       "35    None\n",
       "36    None\n",
       "37    None\n",
       "38    None\n",
       "39    None\n",
       "40    None\n",
       "41    None\n",
       "42    None\n",
       "43    None\n",
       "44    None\n",
       "45    None\n",
       "46    None\n",
       "47    None\n",
       "48    None\n",
       "49    None\n",
       "50    None\n",
       "51    None\n",
       "52    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define path to new annotation files\n",
    "dst_path = \"../datasets/automated_extraction/\"\n",
    "\n",
    "# Generate annotation files for the training set\n",
    "df.apply(utils.write_ann_file, path=dst_path, axis=1)\\\n",
    "\n",
    "# Generate annotation files for the test set\n",
    "df_test.apply(utils.write_ann_file, path=dst_path, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75ef6e",
   "metadata": {},
   "source": [
    "> We use ` brat-eval` (https://github.com/READ-BioMed/brateval) to compare extracted concepts and composite concepts (`\"../datasets/automated_extraction/\"`) with gold standard annotations (`\"../datasets/annotations_composite/\"`).\n",
    ">\n",
    "> We run `brat-eval` on individual folds to obtain cross-validation performance. We then run `brat-eval` on the test set to evaluate the automated extraction of IFI-related information on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8273b",
   "metadata": {},
   "source": [
    "___\n",
    "# Evaluate the automated extraction of IFI-related information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca3df3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .csv file containing the output of brat-eval\n",
    "df = pd.read_csv(\"../datasets/brat_eval_output.csv\", index_col=0)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = utils.get_feature_names(\"concepts\") + utils.get_feature_names(\"composite\")\n",
    "df = df.loc[feature_names]\n",
    "\n",
    "# Columns containing the results for cross-validation folds\n",
    "folds = df.columns[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb75f3",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e75b7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_results(x, i):\n",
    "    try:\n",
    "        m = int(x.split('|')[i])\n",
    "        return m\n",
    "    except:\n",
    "        return np.nan \n",
    "\n",
    "def precision(tp, fp):\n",
    "    return tp / (tp+fp)\n",
    "\n",
    "def recall(tp, fn):\n",
    "    return tp / (tp+fn)\n",
    "\n",
    "def fscore(p, r):\n",
    "    return 2 * (p * r) / (p + r)\n",
    "\n",
    "def print_metrics(rows, cols):\n",
    "    prec = precision(TPs.loc[rows, cols], FPs.loc[rows, cols])\n",
    "    rec = recall(TPs.loc[rows, cols], FNs.loc[rows, cols])\n",
    "    f = fscore(prec, rec)\n",
    "    \n",
    "    if len(cols) > 1: \n",
    "        print(\"Precision:\\n\", prec.aggregate(['mean', 'std'], axis=1).round(2))\n",
    "        print(\"\\nRecall:\\n\", rec.aggregate(['mean', 'std'], axis=1).round(2))\n",
    "        print(\"\\nF1 score:\\n\", f.aggregate(['mean', 'std'], axis=1).round(2))\n",
    "    else:\n",
    "        print(\"Precision:\\n\", prec.round(2))\n",
    "        print(\"\\nRecall:\\n\", rec.round(2))\n",
    "        print(\"\\nF1 score:\\n\", f.round(2))\n",
    "\n",
    "        \n",
    "def print_micro_metrics(rows, cols):\n",
    "    prec = precision(TPs.loc[rows, cols].sum(), FPs.loc[rows, cols].sum())\n",
    "    rec = recall(TPs.loc[rows, cols].sum(), FNs.loc[rows, cols].sum())\n",
    "    f = fscore(prec, rec)\n",
    "    \n",
    "    if len(cols) > 1: \n",
    "        print(\"micro-Precision:\\n\", prec.aggregate(['mean', 'std']).round(2))\n",
    "        print(\"\\nmicro-Recall:\\n\", rec.aggregate(['mean', 'std']).round(2))\n",
    "        print(\"\\nmicro-F1 score:\\n\", f.aggregate(['mean', 'std']).round(2))\n",
    "    else:\n",
    "        print(\"micro-Precision:\\n\", prec.round(2))\n",
    "        print(\"\\nmicro-Recall:\\n\", rec.round(2))\n",
    "        print(\"\\nmicro-F1 score:\\n\", f.round(2))\n",
    "        \n",
    "        \n",
    "def print_macro_metrics(rows, cols):\n",
    "    prec = precision(TPs.loc[rows, cols], FPs.loc[rows, cols])\n",
    "    rec = recall(TPs.loc[rows, cols], FNs.loc[rows, cols])\n",
    "    f = fscore(prec, rec)\n",
    "    \n",
    "    if len(cols) > 1: \n",
    "        print(\"macro-Precision:\\n\", prec.mean(axis=1).mean().round(2))\n",
    "        print(\"\\nmacro-Recall:\\n\", rec.mean(axis=1).mean().round(2))\n",
    "        print(\"\\nmacro-F1 score:\\n\", f.mean(axis=1).mean().round(2))\n",
    "    else:\n",
    "        print(\"macro-Precision:\\n\", prec.mean().round(2))\n",
    "        print(\"\\nmacro-Recall:\\n\", rec.mean().round(2))\n",
    "        print(\"\\nmacro-F1 score:\\n\", f.mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed4c7d",
   "metadata": {},
   "source": [
    "### Parse `brat-eval` output to extract the number of TPs, FPs, and FNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eff970c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold1      463.0\n",
       "fold2      512.0\n",
       "fold3      367.0\n",
       "fold4      456.0\n",
       "fold5      422.0\n",
       "fold6      673.0\n",
       "fold7      735.0\n",
       "fold8      394.0\n",
       "fold9      475.0\n",
       "fold10     461.0\n",
       "TEST      1068.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the number of TPs, FPs, FNs\n",
    "TPs = df.applymap(extract_results, i=-6)\n",
    "FPs = df.applymap(extract_results, i=-5)  \n",
    "FNs = df.applymap(extract_results, i=-4) \n",
    "\n",
    "# Print the total number of concepts detected in each fold\n",
    "(TPs + FPs).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf4fc957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IFI concepts: ['ClinicalQuery', 'FungalDescriptor', 'Fungus', 'Invasiveness', 'Stain']\n",
      "Certainty cues: ['positive', 'equivocal', 'negative']\n",
      "Affirmed concepts: ['affirmedFungalDescriptor', 'affirmedFungus', 'affirmedInvasiveness', 'affirmedStain']\n",
      "Negated concepts: ['negatedFungalDescriptor', 'negatedFungus', 'negatedInvasiveness', 'negatedStain']\n"
     ]
    }
   ],
   "source": [
    "# Group feature names to evaluate them separately\n",
    "ifi_concepts = feature_names[:5]\n",
    "certainty_cues = feature_names[6:9]\n",
    "affirmed_concepts = feature_names[9:13]\n",
    "negated_concepts = feature_names[13:17]\n",
    "print(\"IFI concepts:\", ifi_concepts)\n",
    "print(\"Certainty cues:\", certainty_cues)\n",
    "print(\"Affirmed concepts:\", affirmed_concepts)\n",
    "print(\"Negated concepts:\", negated_concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c693bc72",
   "metadata": {},
   "source": [
    "### CV and test set agreement for each feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9ce4b66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS-VALIDATION\n",
      "Precision:\n",
      "                           mean   std\n",
      "feature                             \n",
      "ClinicalQuery             0.92  0.13\n",
      "FungalDescriptor          0.75  0.10\n",
      "Fungus                    0.82  0.30\n",
      "Invasiveness              0.45  0.41\n",
      "Stain                     0.94  0.05\n",
      "SampleType                0.15  0.03\n",
      "positive                  0.04  0.02\n",
      "equivocal                 0.01  0.02\n",
      "negative                  0.14  0.04\n",
      "affirmedFungalDescriptor  0.64  0.20\n",
      "affirmedFungus            0.76  0.27\n",
      "affirmedInvasiveness      0.03  0.08\n",
      "affirmedStain             0.02  0.06\n",
      "negatedFungalDescriptor   0.78  0.12\n",
      "negatedFungus             0.97  0.07\n",
      "negatedInvasiveness       0.80  0.28\n",
      "negatedStain              0.02  0.05\n",
      "\n",
      "Recall:\n",
      "                           mean   std\n",
      "feature                             \n",
      "ClinicalQuery             0.53  0.35\n",
      "FungalDescriptor          0.93  0.04\n",
      "Fungus                    0.92  0.15\n",
      "Invasiveness              0.60  0.39\n",
      "Stain                     0.95  0.09\n",
      "SampleType                0.86  0.10\n",
      "positive                  0.83  0.17\n",
      "equivocal                 0.58  0.50\n",
      "negative                  0.98  0.05\n",
      "affirmedFungalDescriptor  0.72  0.20\n",
      "affirmedFungus            0.73  0.40\n",
      "affirmedInvasiveness      1.00   NaN\n",
      "affirmedStain             1.00   NaN\n",
      "negatedFungalDescriptor   0.97  0.05\n",
      "negatedFungus             0.93  0.17\n",
      "negatedInvasiveness       0.67  0.58\n",
      "negatedStain              0.67  0.58\n",
      "\n",
      "F1 score:\n",
      "                           mean   std\n",
      "feature                             \n",
      "ClinicalQuery             0.68  0.27\n",
      "FungalDescriptor          0.83  0.07\n",
      "Fungus                    0.90  0.09\n",
      "Invasiveness              0.71  0.25\n",
      "Stain                     0.94  0.05\n",
      "SampleType                0.26  0.04\n",
      "positive                  0.08  0.03\n",
      "equivocal                 0.07  0.01\n",
      "negative                  0.24  0.06\n",
      "affirmedFungalDescriptor  0.66  0.18\n",
      "affirmedFungus            0.72  0.27\n",
      "affirmedInvasiveness      0.33   NaN\n",
      "affirmedStain             0.29   NaN\n",
      "negatedFungalDescriptor   0.86  0.09\n",
      "negatedFungus             0.94  0.11\n",
      "negatedInvasiveness       0.88  0.18\n",
      "negatedStain              0.19  0.09\n",
      "________________________________________________________________________________ \n",
      "TEST\n",
      "Precision:\n",
      "                           TEST\n",
      "feature                       \n",
      "ClinicalQuery             1.00\n",
      "FungalDescriptor          0.68\n",
      "Fungus                    0.88\n",
      "Invasiveness              0.33\n",
      "Stain                     1.00\n",
      "SampleType                0.14\n",
      "positive                  0.03\n",
      "equivocal                 0.00\n",
      "negative                  0.15\n",
      "affirmedFungalDescriptor  0.40\n",
      "affirmedFungus            1.00\n",
      "affirmedInvasiveness      0.00\n",
      "affirmedStain             0.00\n",
      "negatedFungalDescriptor   0.86\n",
      "negatedFungus             1.00\n",
      "negatedInvasiveness        NaN\n",
      "negatedStain              0.05\n",
      "\n",
      "Recall:\n",
      "                           TEST\n",
      "feature                       \n",
      "ClinicalQuery             0.69\n",
      "FungalDescriptor          0.93\n",
      "Fungus                    0.94\n",
      "Invasiveness              0.12\n",
      "Stain                     1.00\n",
      "SampleType                0.75\n",
      "positive                  0.73\n",
      "equivocal                  NaN\n",
      "negative                  0.90\n",
      "affirmedFungalDescriptor  0.67\n",
      "affirmedFungus            0.40\n",
      "affirmedInvasiveness       NaN\n",
      "affirmedStain             0.00\n",
      "negatedFungalDescriptor   0.93\n",
      "negatedFungus             1.00\n",
      "negatedInvasiveness       0.00\n",
      "negatedStain              1.00\n",
      "\n",
      "F1 score:\n",
      "                           TEST\n",
      "feature                       \n",
      "ClinicalQuery             0.81\n",
      "FungalDescriptor          0.79\n",
      "Fungus                    0.91\n",
      "Invasiveness              0.18\n",
      "Stain                     1.00\n",
      "SampleType                0.24\n",
      "positive                  0.05\n",
      "equivocal                  NaN\n",
      "negative                  0.26\n",
      "affirmedFungalDescriptor  0.50\n",
      "affirmedFungus            0.57\n",
      "affirmedInvasiveness       NaN\n",
      "affirmedStain              NaN\n",
      "negatedFungalDescriptor   0.89\n",
      "negatedFungus             1.00\n",
      "negatedInvasiveness        NaN\n",
      "negatedStain              0.09\n"
     ]
    }
   ],
   "source": [
    "# Print precision, recall, and f-score for every feature across CV folds\n",
    "print(\"CROSS-VALIDATION\")\n",
    "print_metrics(feature_names, folds)\n",
    "\n",
    "# Print precision, recall, and f-score for every feature in the test set\n",
    "print(\"_\"*80, \"\\nTEST\")\n",
    "print_metrics(feature_names, ['TEST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3bb0e",
   "metadata": {},
   "source": [
    "### CV and test set micro-average for IFI concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba06e5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS-VALIDATION\n",
      "micro-Precision:\n",
      " mean    0.81\n",
      "std     0.07\n",
      "dtype: float64\n",
      "\n",
      "micro-Recall:\n",
      " mean    0.87\n",
      "std     0.09\n",
      "dtype: float64\n",
      "\n",
      "micro-F1 score:\n",
      " mean    0.84\n",
      "std     0.07\n",
      "dtype: float64\n",
      "________________________________________________________________________________ \n",
      "TEST\n",
      "micro-Precision:\n",
      " TEST    0.8\n",
      "dtype: float64\n",
      "\n",
      "micro-Recall:\n",
      " TEST    0.86\n",
      "dtype: float64\n",
      "\n",
      "micro-F1 score:\n",
      " TEST    0.83\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print micro-precision, -recall, and -f-score for IFI features across CV fold\n",
    "print(\"CROSS-VALIDATION\")\n",
    "print_micro_metrics(ifi_concepts, folds)\n",
    "\n",
    "# Print micro-precision, -recall, and -f-score for IFI features on the test set\n",
    "print(\"_\"*80, \"\\nTEST\")\n",
    "print_micro_metrics(ifi_concepts, ['TEST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1335b",
   "metadata": {},
   "source": [
    "### CV and test set micro-average for certainty cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f4ac910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS-VALIDATION\n",
      "micro-Precision:\n",
      " mean    0.07\n",
      "std     0.02\n",
      "dtype: float64\n",
      "\n",
      "micro-Recall:\n",
      " mean    0.89\n",
      "std     0.08\n",
      "dtype: float64\n",
      "\n",
      "micro-F1 score:\n",
      " mean    0.12\n",
      "std     0.03\n",
      "dtype: float64\n",
      "________________________________________________________________________________ \n",
      "TEST\n",
      "micro-Precision:\n",
      " TEST    0.06\n",
      "dtype: float64\n",
      "\n",
      "micro-Recall:\n",
      " TEST    0.84\n",
      "dtype: float64\n",
      "\n",
      "micro-F1 score:\n",
      " TEST    0.11\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print micro-precision, -recall, and -f-score for certainty cues across CV fold\n",
    "print(\"CROSS-VALIDATION\")\n",
    "print_micro_metrics(certainty_cues, folds)\n",
    "\n",
    "# Print micro-precision, -recall, and -f-score for certainty cues on the test set\n",
    "print(\"_\"*80, \"\\nTEST\")\n",
    "print_micro_metrics(certainty_cues, ['TEST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7fada6",
   "metadata": {},
   "source": [
    "### CV and test set micro-average for affirmed concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c96a442c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS-VALIDATION\n",
      "micro-Precision:\n",
      " mean    0.48\n",
      "std     0.16\n",
      "dtype: float64\n",
      "\n",
      "micro-Recall:\n",
      " mean    0.69\n",
      "std     0.24\n",
      "dtype: float64\n",
      "\n",
      "micro-F1 score:\n",
      " mean    0.55\n",
      "std     0.17\n",
      "dtype: float64\n",
      "________________________________________________________________________________ \n",
      "TEST\n",
      "micro-Precision:\n",
      " TEST    0.37\n",
      "dtype: float64\n",
      "\n",
      "micro-Recall:\n",
      " TEST    0.56\n",
      "dtype: float64\n",
      "\n",
      "micro-F1 score:\n",
      " TEST    0.44\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print micro-precision, -recall, and -f-score for affirmed features across CV fold\n",
    "print(\"CROSS-VALIDATION\")\n",
    "print_micro_metrics(affirmed_concepts, folds)\n",
    "\n",
    "# Print micro-precision, -recall, and -f-score for affirmed features on the test set\n",
    "print(\"_\"*80, \"\\nTEST\")\n",
    "print_micro_metrics(affirmed_concepts, ['TEST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe46b7",
   "metadata": {},
   "source": [
    "### CV and test set micro-average for negated concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5326b88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS-VALIDATION\n",
      "micro-Precision:\n",
      " mean    0.57\n",
      "std     0.09\n",
      "dtype: float64\n",
      "\n",
      "micro-Recall:\n",
      " mean    0.96\n",
      "std     0.07\n",
      "dtype: float64\n",
      "\n",
      "micro-F1 score:\n",
      " mean    0.71\n",
      "std     0.08\n",
      "dtype: float64\n",
      "________________________________________________________________________________ \n",
      "TEST\n",
      "micro-Precision:\n",
      " TEST    0.6\n",
      "dtype: float64\n",
      "\n",
      "micro-Recall:\n",
      " TEST    0.92\n",
      "dtype: float64\n",
      "\n",
      "micro-F1 score:\n",
      " TEST    0.73\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print micro-precision, -recall, and -f-score for affirmed features across CV fold\n",
    "print(\"CROSS-VALIDATION\")\n",
    "print_micro_metrics(negated_concepts, folds)\n",
    "\n",
    "# Print micro-precision, -recall, and -f-score for affirmed features on the test set\n",
    "print(\"_\"*80, \"\\nTEST\")\n",
    "print_micro_metrics(negated_concepts, ['TEST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a308f33d",
   "metadata": {},
   "source": [
    "### CV and test set macro-average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68fbaeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS-VALIDATION\n",
      "macro-Precision:\n",
      " 0.49\n",
      "\n",
      "macro-Recall:\n",
      " 0.81\n",
      "\n",
      "macro-F1 score:\n",
      " 0.56\n",
      "________________________________________________________________________________ \n",
      "TEST\n",
      "macro-Precision:\n",
      " TEST    0.47\n",
      "dtype: float64\n",
      "\n",
      "macro-Recall:\n",
      " TEST    0.67\n",
      "dtype: float64\n",
      "\n",
      "macro-F1 score:\n",
      " TEST    0.56\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print macro-precision, -recall, and -f-score for all concepts across CV fold\n",
    "print(\"CROSS-VALIDATION\")\n",
    "print_macro_metrics(feature_names, folds)\n",
    "\n",
    "# Print macro-precision, -recall, and -f-score for all concepts on the test set\n",
    "print(\"_\"*80, \"\\nTEST\")\n",
    "print_macro_metrics(feature_names, ['TEST'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
